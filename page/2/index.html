<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>mdyao's Blog</title><meta name="author" content="mdyao"><meta name="copyright" content="mdyao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="mdyao&#39;s Blog">
<meta property="og:url" content="https://mdyao.github.io/page/2/index.html">
<meta property="og:site_name" content="mdyao&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:author" content="mdyao">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://mdyao.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="tzbP2QdXJut36Na1bUJBMHziIAtYcK406Vw6uUypl50"/><meta name="baidu-site-verification" content="code-eaYP8YhC0u"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?275a3bd1ff797bd6f6388b7ee2e8c131";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'mdyao\'s Blog',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-07-09 16:48:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/wallroom.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">mdyao's Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">mdyao's Blog</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/mdyao" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mingdeyao@foxmai.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.zhihu.com/people/you-men-de" target="_blank" title="Zhihu"><i class="fab fa-zhihu"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/paper-uemori-skin-based-identification-from-multispectral-image-data-using-cnns-cvpr-2019/" title="论文笔记|Skin-based identification from multispectral image data using CNNs">论文笔记|Skin-based identification from multispectral image data using CNNs</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2019-06-21T07:51:00.000Z" title="Created 2019-06-21 15:51:00">2019-06-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/">文章阅读</a></span></div><div class="content">写在前面：此文发于CVPR2019，欢迎在文末留言交流。
题目：Skin-based identification from multispectral image data using CNNs作者：Takeshi Uemori1 Atsushi Ito2 Yusuke Moriuchi2 Alexander Gatto1 Jun Murayama21Sony Europe B.V., Stuttgart, Germany 2Sony Corporation, Tokyo, Japan







作者提出一种仅利用一块皮肤的高光谱图像进行生物识别的方法，是“首个使用手掌描述了位姿不变和对遮挡鲁棒的实时人体识别系统”。
根据文献中的光学考虑，感知到的颜色主要由真皮散射、黑色素和血管吸收组成，由于皮肤生色团浓度的不同，这些颜色在个体之间是不同的。
利用手的一个Patch(16*16)，作者的CNN模型可以对注册用户进行识别，而不使用手的形状作为额外信息。此模型也可以分辨左手右手+实时。
此外，作者探究了马赛克阵列光谱相机中 光谱与空间维度的trade-off。即在固定三维体积(光谱+ ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/paper-hyperspectral-image-super-resolution-with-optimized-rgb-guidance-cvpr-2019/" title="论文笔记|Hyperspectral Image Super-Resolution with Optimized RGB Guidance">论文笔记|Hyperspectral Image Super-Resolution with Optimized RGB Guidance</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2019-06-19T12:49:00.000Z" title="Created 2019-06-19 20:49:00">2019-06-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/">文章阅读</a></span></div><div class="content">写在前面：此文发于CVPR2019。
题目：Hyperspectral Image Super-Resolution with Optimized RGB Guidance作者：Ying Fu1 Tao Zhang1 Yinqiang Zheng2 Debing Zhang3 Hua Huang1Beijing Institute of Technology 2National Institute of Informatics 3DeepGlint



本文提出了非监督高光谱超分辨的网络结构，并加入了CSR优化层，去选择特定的优化。
作者提出来一种端到端的RGB引导高光谱超分辨的方法，可以有效地估计RGB空间和高光谱空间的非线性映射，并且利用了空间一致性。除此之外，作者还提出CSR优化层去根据给定的CSR数据集选择最优的CSR，甚至可以设计一个有利于优化RGB引导HSI超分辨任务的新CSR函数。
建模有低分辨率HSI$X_l$，高分辨率RGB$Y$，和待恢复HSI数据$X$。因此有$$X_l&#x3D;XH,~Y&#x3D;CX\tag{1}$$其中$H$为空间下采样，$C$为CSR ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/paper-multispectral-and-hyperspectral-image-fusion-by-mshs-fusion-net-cvpr-2019/" title="论文笔记|Multispectral and Hyperspectral Image Fusion by MS/HS Fusion Net">论文笔记|Multispectral and Hyperspectral Image Fusion by MS/HS Fusion Net</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2019-06-19T12:49:00.000Z" title="Created 2019-06-19 20:49:00">2019-06-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/">文章阅读</a></span></div><div class="content">写在前面：此文发于CVPR2019，欢迎在文末留言交流。
题目：Multispectral and Hyperspectral Image Fusion by MS&#x2F;HS Fusion Net作者：Qi Xie1, Minghao Zhou1, Qian Zhao1, Deyu Meng1,∗ , Wangmeng Zuo2, Zongben Xu1Xi’an Jiaotong University; 2Harbin Institute of Technology


作者提出了一种融合HR-MS和LR-HS的方法，从而生成HR-HS。作者提出的想融合模型考虑了低分辨率图像的观测模型和HR-HS的低秩特性。随后作者设计了一种迭代优化方法，并利用近端梯度法(Proximal Gradient Method, PG)求解此模型。最后作者将以上模型表示成深度网络形式(MS&#x2F;HS Fusion Net)，通过卷积神经网络学习近端(proximal)操作子和模型参数。
直接恢复高分辨率的高光谱图像是一个病态的逆问题，通常操作是赋予先验。如，1、利用HR-MS训练的字典可以稀疏 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/paper-hyperspectral-image-reconstruction-using-a-deep-spatial-spectral-prior-cvpr-2019/" title="论文笔记|Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior">论文笔记|Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2019-06-18T15:07:00.000Z" title="Created 2019-06-18 23:07:00">2019-06-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/">文章阅读</a></span></div><div class="content">写在前面：此文发于CVPR2019，欢迎在文末留言讨论。
题目：Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior作者：Lizhi Wang1 Chen Sun1 Ying Fu1 Min H. Kim2 Hua Huang11Beijing Institute of Technology 2Korea Advanced Institute of Science and Technology



正则化是求解病态优化问题的一个基本方法，并且在高光谱图像重建中得到了广泛应用。但是以往基于正则化的方法往往需要手工设计且对变化范围大的场景不鲁棒。作者提出了一种结合数据驱动先验和基于优化网络的高光谱图像重建方案，在仿真和实物系统上达到了较高精度。
为了解决欠定重建问题，可以使用正则化引入图像先验，如全变分(TV)，sparsity,non-local similarity等正则化方法。但基于经验设计的正则化方法对多样的自然光谱图像处理能力差，需要手工微调参数且往往不能求取闭合解。近期提出的基于神经网络的 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/2019-04-15-valse2019/" title="小记|Valse2019">小记|Valse2019</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2019-04-15T15:35:21.000Z" title="Created 2019-04-15 23:35:21">2019-04-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></span></div><div class="content">OverviewValse 2019
本次valse注册会议的人数达到了5000+，现场也异常火爆，许多workshop&#x2F;tutorial开场前10min会场就已经坐满。而第一天的三维与深度学习的workshop则因为场地太小，主办方在已经开讲的情况下转移会场。这场汇集了华人CV青年学者的研讨会是当前深度学习与CV领域的一个缩影，异常火爆与拥挤，但也百花齐放，成果丰硕，不断碰撞出新的花火。
本届valse主题非常丰富，几乎包含了计算机视觉领域的所有热点，以弱监督、迁移学习、元学习、模型与网络设计、医学影像、三维重建为代表的主题是学界关注的重点。从公司workshop和展区来看，当前工业界并不怎么关注弱监督、迁移学习等概念，业界更关注CV与AI的直接产业化，如轻量级网络的设计，网络模型搜索，行人识别、语义分析、三维场景地图等与产品更近的技术）。


会议的很多workshop是同步进行的，所以有必要根据自己的兴趣主动做出取舍。当然，有时是因为会场爆满“被动”去听另一个完全听不懂的workshop。印象较深与略有收获主要集中在第一天与最后一天：第一天workshop“三维视觉与深 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2018/2018-11-24-opengl/" title="Linux运行OpenGL">Linux运行OpenGL</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-11-24T14:38:00.000Z" title="Created 2018-11-24 22:38:00">2018-11-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/OpenGL/">OpenGL</a></span></div><div class="content">写在前面：在linux下开发OpenGL。简单介绍OpenGL并安装，编写第一个程序。
环境：Ubuntu 16.04 LTSGraphics:GT 730gcc 5.4.0


什么是OpenGLOpenGL 是一套由SGI公司发展出来的绘图函式库，它是一组 C 语言的函式，用于 2D 与 3D 图形应用程式的开发上。OpenGL 让程式开发人员不需要考虑到各种显示卡底层运作是否相同的问题，硬体由 OpenGL 核心去沟通，因此只要显示卡支援 OpenGL，那么程式就不需要重新再移植，而程式开发人员也不需要重新学习一组函式库来移植程式。
安装1.首先安装编译器与基本函数库，一般情况下系统均已安装：
1$ sudo apt-get install build-essential

2.其次安装 OpenGL Library
1$ sudo apt-get install libgl1-mesa-dev

3.安装OpenGL Utilities
1$ sudo apt-get install libglu1-mesa-dev

OpenGL Utilities 是一组建构于 OpenG ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2018/2018-11-16-cuda/" title="CUDA程序开发">CUDA程序开发</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-11-16T14:38:00.000Z" title="Created 2018-11-16 22:38:00">2018-11-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/CUDA/">CUDA</a></span></div><div class="content">写在前面：GPU并行计算和CUDA程序开发及优化 课堂笔记+课程作业主要是一些小程序练手


GPU相关知识GPU的计算模式在异构协同处理计算模型中将CPU与GPU结合起来加以利用。应用程序的串行部分在CPU上运行，而计算任务繁重的部分则由GPU的高性能计算来进行。从用户的角度来看，应用程序只是运行得更快了，获得了很好的性能提升。
高性能计算机的 分类单指令流单数据流（ SISD）• 单指令流多数据流（ SIMD）• 多指令流单数据流（ MISD）• 多指令流多数据流（ MIMD）
单独的高性能计算节点主要分为：• 同构节点（仅采用CPU， Intel Xeon CPU、 AMD Opteron CPU）• 异构节点（分为主机端和设备端，分别注重逻辑运算和浮点计算。
主流异构节点类型包括CPU+GPU和CPU+MICMIC： Many Integrated Core （Intel 集成众核）
GPU硬件架构NVIDIA不同架构产品不同GPU架构的设计理念、工艺水平等均不相同，相应的内部体系结构和性能也不相同。每一构架都对应大量产品

Tesla
Fermi
Kepler
Maxwell ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2018/paper-deep3d/" title="论文笔记|DEEP3D">论文笔记|DEEP3D</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-11-07T09:10:00.000Z" title="Created 2018-11-07 17:10:00">2018-11-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/">文章阅读</a></span></div><div class="content">写在前面：Deep3d,由2D到3D视频转换，非常经典的视角重建。有码。论文：Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks，作者：Junyuan Xie, Ross Girshick, Ali FarhadiUniversity of Washington


此文并不是先预测一张深度图，然后用这张深度图通过一个单独的算法去重建缺失的视角，而是在同一神经网络中重新创建端到端的方法来训练它。
ReferenceDeep3D 中文翻译及阅读笔记开源|如何使用CNN将视频从2D到3D进行自动转换（附源代码）
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2018/paper-sr-summary/" title="论文笔记：SR总结">论文笔记：SR总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-11-07T09:10:00.000Z" title="Created 2018-11-07 17:10:00">2018-11-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/">文章阅读</a></span></div><div class="content">写在前面：这是一篇不太完善的CNN超分辨总结，整理了近年来深度学习在超分辨领域比较有代表性的工作，随缘更新。




SRCNN
(有码)
论文：

Learning a Deep Convolutional Network for Image Super-Resolution, ECCV2014
Image Super-Resolution Using Deep Convolutional Networks, TPAMI2015

项目主页：http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html作者：Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang.
VDSR
论文：Accurate Image Super-Resolution Using Very Deep Convolutional Networks, CVPR2016代码：

official: https://cv.snu.ac.kr/research/VDSR/
pytorch: https://github.com/twtygq ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2018/paper-monodepth/" title="论文笔记|Unsupervised Monocular Depth Estimation with Left-Right Consistency">论文笔记|Unsupervised Monocular Depth Estimation with Left-Right Consistency</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-11-07T09:10:00.000Z" title="Created 2018-11-07 17:10:00">2018-11-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/">文章阅读</a></span></div><div class="content">写在前面：此文发于CVPR2017，并在项目主页公布了代码。本文中文翻译：读Unsupervised Monocular Depth Estimation with LeftRight Consistency代码中文解读：[读源码] Unsupervised Monocular Depth Estimation with Left-Right Consistency关于近几年单目深度估计的文章，可以参考知乎用户的回答。
题目：Unsupervised Monocular Depth Estimation with Left-Right Consistency作者：Clement Godard     Oisin Mac Aodha     Gabriel J. BrostowUniversity College London


背景motivation思考：人眼可以做到单目的原因是什么？
回答：人眼单目深度估计是基于极强的先验，这也限制了单目深度估计的应用场景。
“利用图像重建误差（image reconstruction loss）来最小化光度误差（类似于SLAM中的直接法）虽可以 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">mdyao</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://www.zhihu.com/people/you-men-de"><i class="fab fa-zhihu"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mdyao" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mingdeyao@foxmai.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.zhihu.com/people/you-men-de" target="_blank" title="Zhihu"><i class="fab fa-zhihu"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/paper-202001-pirm2018challenge/" title="论文笔记|The 2018 PIRM Challenge on Perceptual Image Super-resolution">论文笔记|The 2018 PIRM Challenge on Perceptual Image Super-resolution</a><time datetime="2020-01-10T06:00:00.000Z" title="Created 2020-01-10 14:00:00">2020-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2019/paper-201911-pro-cam-ssfm-projector-camera-system-for-structure-and-spectral-reflectance-from-motion/" title="论文笔记|Pro-Cam SSfM Projector-Camera System for Structure and Spectral Reflectance From Motion">论文笔记|Pro-Cam SSfM Projector-Camera System for Structure and Spectral Reflectance From Motion</a><time datetime="2019-11-30T13:04:00.000Z" title="Created 2019-11-30 21:04:00">2019-11-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2019/paper-201911-deep-blind-hyperspectral-image-fusion/" title="论文笔记|Deep Blind Hyperspectral Image Fusion">论文笔记|Deep Blind Hyperspectral Image Fusion</a><time datetime="2019-11-28T15:23:00.000Z" title="Created 2019-11-28 23:23:00">2019-11-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2019/paper-201910-rgb-to-d/" title="论文笔记|Neural RGB-D Sensing Depth and Uncertainty from a Video Camera">论文笔记|Neural RGB-D Sensing Depth and Uncertainty from a Video Camera</a><time datetime="2019-10-28T06:45:00.000Z" title="Created 2019-10-28 14:45:00">2019-10-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2019/paper-201910-extreme-view-synthesis/" title="论文笔记|Extreme View Synthesis">论文笔记|Extreme View Synthesis</a><time datetime="2019-10-14T14:14:00.000Z" title="Created 2019-10-14 22:14:00">2019-10-14</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/CUDA/"><span class="card-category-list-name">CUDA</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/OpenGL/"><span class="card-category-list-name">OpenGL</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/hexo/"><span class="card-category-list-name">hexo</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/linux/"><span class="card-category-list-name">linux</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"><span class="card-category-list-name">图像处理</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/"><span class="card-category-list-name">文章阅读</span><span class="card-category-list-count">19</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="card-category-list-name">深度学习</span><span class="card-category-list-count">3</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/CUDA/" style="font-size: 1.1em; color: #999">CUDA</a> <a href="/tags/MATLAB/" style="font-size: 1.1em; color: #999">MATLAB</a> <a href="/tags/OpenGL/" style="font-size: 1.1em; color: #999">OpenGL</a> <a href="/tags/XJBX/" style="font-size: 1.42em; color: #99a6b7">XJBX</a> <a href="/tags/linux/" style="font-size: 1.1em; color: #999">linux</a> <a href="/tags/%E5%85%89%E8%B0%B1/" style="font-size: 1.42em; color: #99a6b7">光谱</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 1.34em; color: #99a3b0">图像处理</a> <a href="/tags/%E6%91%B8%E9%B1%BC%E6%97%A5%E5%BF%97/" style="font-size: 1.26em; color: #999fa8">摸鱼日志</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 1.1em; color: #999">深度估计</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BF%A1%E6%81%AF/" style="font-size: 1.26em; color: #999fa8">深度信息</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">深度学习</a> <a href="/tags/%E8%A7%86%E8%A7%92%E5%90%88%E6%88%90/" style="font-size: 1.18em; color: #999ca1">视角合成</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 1.5em; color: #99a9bf">论文笔记</a> <a href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8/" style="font-size: 1.1em; color: #999">超分辨</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/01/"><span class="card-archive-list-date">January 2020</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2019/11/"><span class="card-archive-list-date">November 2019</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2019/10/"><span class="card-archive-list-date">October 2019</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2019/06/"><span class="card-archive-list-date">June 2019</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2019/04/"><span class="card-archive-list-date">April 2019</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2018/11/"><span class="card-archive-list-date">November 2018</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2018/10/"><span class="card-archive-list-date">October 2018</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2018/08/"><span class="card-archive-list-date">August 2018</span><span class="card-archive-list-count">3</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">35</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-07-09T08:48:40.674Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By mdyao</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>