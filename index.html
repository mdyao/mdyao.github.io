<html>
<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Mingde Yao (姚明德)</title>
  <meta content="Mingde Yao (姚明德), mdyao.github.io" name="keywords" />
  <style media="screen" type="text/css">
  html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  /* font-family: Lato, sans-serif; */
  font-family: Arial;
  /* font-family: Georgia; */
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #043d98;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}



* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 2em auto 2em auto;
  width: 870px;
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 15px;
  background: #F4F6F6;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  /* font-family: Arial, Verdana, Helvetica, sans-serif; */
  /* font-size: 13px; */
  font-weight:bold;
}

ul { 
  /* list-style: circle; */
  list-style: disc;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Arial, Helvetica, sans-serif;
  font-size: 14px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
  font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.2em;
  background: #F4F6F6;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 2px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}
div.paper2 {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

div.experience {
  /* clear: both; */
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  /* border: 2px solid #ddd; */
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  /* border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px; */
  line-height: 140%;
}


div.education {
  /* clear: both; */
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  /* border: 2px solid #ddd; */
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  /* border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px; */
  line-height: 140%;
}


div.paper:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}

div.paper2:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}
div.bio {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 135%;
}

div.res {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.award {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.paper div {
  padding-left: 270px;
}

div.experience div {
  padding-left: 200px;
}

div.education div {
  padding-left: 200px;
}

img.paper {
  /* margin-bottom: 0.4em; */
  float: left;
  width: 250px;

}

img.experience {
  /* margin-bottom: 0.4em; */
  padding-top: 10px;
  float: left;
  width: 170px;

}

img.education {
  /* margin-bottom: 0.4em; */
  padding-left: 30px;
  float: left;
  width: 100px;

}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 14px;
  margin: 1em 0;
  padding: 0;
}

    .bot {
  font-size: 14%;
}

   .ptypej {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #5cb85c;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypec {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #428bca;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypep {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #6B6B6B;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
/* navigation */
#nav {
  /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',
       Corbel, Arial, Helvetica, sans-serif; */
  font-family: Georgia, Helvetica, sans-serif;
  position: fixed;
  top: 50px;
  /* left: 860px; */
  margin-left: 860px;     /*1060*/
  width: 92px;
  font-size: 15px;
}

#nav li2 {
    margin-bottom: 1px;
}
ol {
  list-style: none;
}
#nav a {
    display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none;
}

#nav a:hover {
    color: #ffde00;
    /* background-color: #242d36 ; */
}
</style>

<!-- <script type="text/javascript" async="" src="./files/ga.js"></script>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function () {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script> -->

<script type="text/javascript" src="./files/hidebib.js"></script>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- <script src="./files/main.js"></script> -->

<body>
  <ol id="nav">
    <li><a href="#home" title="Home">Home</a></li>
    <li><a href="#news" title="News">News</a></li>
    <li><a href="#pub" title="Papers">Papers</a></li>
    <li><a href="mailto:wuwenhao17@mails.ucas.edu.cn" title="Contact">Contact</a></li>
  </ol>
<a name="home"></a>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Mingde Yao (姚明德)" style="float: left; padding-left: .01em; height: 140px;" src="whwu_fuji.jpg" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Mingde Yao (姚明德)</span><br />
<!-- <p>&nbsp;</p> -->
<p><a href="https://scholar.google.com/citations?user=Kn5d1ckAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
  <a href="https://github.com/whwu95">Github</a> &nbsp/&nbsp
  <a href="https://www.linkedin.com/in/%E6%96%87%E7%81%8F-%E5%90%B4-aab583128/?locale=en_US">Linkedin</a> &nbsp/&nbsp
  <a href="https://www.zhihu.com/people/wu-wen-hao-80-23">Zhihu</a></p>
<p>&nbsp;</p>

<p><strong>Ph.D. Candidate, The University of Sydney</strong><br /><br />
<!-- <p><strong>Senior R&D Engineer @ Baidu Inc.</strong><br /><br /> -->

<span><strong>Personal Email </strong>: whwu.ucas (at) gmail.com</span> <br />
<span><strong>School Email </strong>: wenhao.wu (at) sydney.edu.au</span> <br />
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<!-- <h2>(<a href='https://scholar.google.com/citations?user=kWADCMUAAAAJ&hl=zh-CN'>Google scholar</a>)</h2> -->
<h2>About Me</h2>
<div class="bio">



Currently, I am a Ph.D. student at <a href="https://www.sydney.edu.au/">University of Science and Technology of China (USTC)</a>, where I am supervised by <a href="http://staff.ustc.edu.cn/~zwxiong/">Prof. Zhiwei Xiong</a> and <a href="https://faculty.ustc.edu.cn/dongeliu/">Dong Liu</a>. I obtained my B. Eng. degree in department of automation, Northeastern University (NEU, China) in 2018.

<!-- <br><br>
My research interests focus on computer vision, especially low-level vision tasks.
 -->
<br><br>
My research interests broadly lie in the areas of computational photography, computational imaging, and deep learning (e.g., super-resolution, denoising, enhancement, high-dynamic-range imaging, inverse tone mapping, and hyper-spectral image processing)
<!-- including <strong style="font-size:15px;color:#8aa371">low-level vision</strong> 
, 
<strong style="font-size:15px;color:#8aa371">Spectral image processing</strong>. -->
<!-- Recently, I have shifted my focus to 
<strong style="font-size:15px;color:#0294b9"></strong> (e.g., combining vision and language) and
<strong style="font-size:15px;color:#0294b9">AI Generated Content (AIGC)</strong>.
 -->


<br><br>
I will join CUHK as a research post-doctor, advised by <a href="https://www.gujinwei.org/">Jinwei Gu</a>.



<br><br>


</div>
</div>


<a name="news"></a>
<div style="clear: both;">
<div class="section">
  <h2>Updates</h2>
  <div class="paper">
    <ul>

    <li><a class="button" href="#" style="color:#FFA500"><strong style="font-size:16px">New</strong></a> <strong
        style="padding-left:5px;">07/2023: </strong>
      <em> One paper (Temporal Modeling: <a href="https://arxiv.org/abs/2307.08908">ATM</a>,
        Cross-Modal Retrieval: <a href="https://arxiv.org/abs/2301.06309">UA</a>) are accepted by <strong><a href="https://iccv2023.thecvf.com/">
            <font color="DarkRed">ICCV2023</font></a></strong>.
        </em>
    </li>



    <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:16px">New</strong></a> <strong
        style="padding-left:5px;">02/2023: </strong>
      <em> <strong>Two</strong> First-author papers (Video Recognition: <a href="https://arxiv.org/abs/2301.00182">BIKE</a>,
        Video Retrieval: <a href="https://arxiv.org/abs/2301.00184">Cap4Video</a>) are accepted by <strong><a href="https://cvpr2023.thecvf.com/">
            <font color="DarkRed">CVPR 2023</font></a></strong>. 
            <a href="https://arxiv.org/abs/2301.00184">Cap4Video</a>, which involves GPT-2 to enhance cross-modal learning, 
            is selected as a <font color="Red"><b>Highlight</b></font> paper (<strong>Top 2.5%</strong>).
        </em>
    </li>

    <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:16px">New</strong></a> <strong
        style="padding-left:5px;">11/2022: </strong>
      <em> <strong>Two</strong> papers (Video Recognition: <a href="https://arxiv.org/pdf/2207.01297.pdf">Text4Vis</a>,
      Style Transfer: <a href="https://arxiv.org/pdf/2212.01567.pdf">AdaCM</a>) are accepted by 
      <strong><a href="https://aaai.org/Conferences/AAAI-23/"><font color="DarkRed">AAAI 2023</font></a></strong>.
        We have released <a href="https://github.com/whwu95/Text4Vis">Codes and Models</a> of Text4Vis.</em>
    </li>

    <li> <strong>07/2022: </strong>
      <em> <strong>Three</strong> papers (Video Sampling: <a href="https://arxiv.org/pdf/2207.10388.pdf">NSNet</a>, <a
        href="https://arxiv.org/pdf/2207.10379.pdf">TSQNet</a>, Cross-Modal Learning: <a
        href="https://arxiv.org/pdf/2208.09843.pdf">CODER</a>) are accepted by
        <strong><a href="https://eccv2022.ecva.net/">
            <font color="DarkRed">ECCV 2022</font></a></strong>.</em>
    </li>

    <li> <strong>06/2022: </strong>
      <em> Our <a href="https://dl.acm.org/doi/10.1145/3503161.3547888">MaMiCo</a>, 
        a new video self-supervised learning work, is accepted by
        <strong><a href="https://2022.acmmm.org/">
            <font color="DarkRed">ACMMM 2022</font>
          </a></strong>(<font color="Red"><b>Oral Presentation</b></font>).</em>
    </li>

    <li> <strong>03/2022: </strong>
      <em> <strong>Two</strong> low-level vision papers (<a href="https://arxiv.org/pdf/2203.12707.pdf">MSPC</a>, <a
        href="https://arxiv.org/pdf/2203.00911.pdf">BAIRNet</a>) are accepted by
        <strong><a href="https://cvpr2022.thecvf.com/">
            <font color="DarkRed">CVPR 2022</font></a></strong>.</em>
    </li>

    <li> <strong>12/2021: </strong>
      <em> Our <a href="https://arxiv.org/pdf/2112.07984.pdf">BCNet</a>, 
        an general temporal localization framework, is accepted by
        <strong><a href="https://aaai.org/Conferences/AAAI-22/">
            <font color="DarkRed">AAAI 2022</font></a></strong>. 
            <a href="https://github.com/happy-hsy/BCNet">Code</a> is available.</em>
    </li>

    <li> <strong>07/2021: </strong>
      <em> Our <a href="https://arxiv.org/pdf/2106.02342.pdf">ASCNet</a>,
        a self-supervised video representation learning framework, is accepted by
        <strong><a href="https://iccv2021.thecvf.com/">
            <font color="DarkRed">ICCV 2021</font></a></strong>.</em>
    </li>

    <li> <strong>07/2021: </strong>
      <em> <strong>Two</strong> papers (Video Recognition, Crowd Counting) are accepted by
        <strong><a href="https://2021.acmmm.org/">
            <font color="DarkRed">ACMMM 2021</font></a></strong>.</em>
    </li>

    <li> <strong>04/2021: </strong>
      <em> We present a novel task: 
        <a href="https://arxiv.org/pdf/2108.03825.pdf">Weakly-Supervised Spatio-Temporal Anomaly Detection</a>,
        is accepted by
        <strong><a href="https://ijcai-21.org/">
            <font color="DarkRed">IJCAI 2021</font></a></strong>.</em>
    </li>


    <li> <strong>04/2021: </strong>
      <em><font color="Red"><b>Winner</b></font> in the Traffic Anomaly Detection Track of the 
        <strong><a href="https://www.aicitychallenge.org/">
        <font color="DarkRed">CVPR 2021 AI CITY CHALLENGE</font></a></strong>.</em></li>


    <li> <strong>12/2020: </strong> 
      <em>Our <a href="https://arxiv.org/pdf/2012.06977.pdf">MVFNet</a>, 
        an efficient temporal module, is accepted by 
        <strong><a href="https://aaai.org/Conferences/AAAI-21/">
            <font color="DarkRed">AAAI 2021</font></a></strong>. 
        <a href="https://github.com/whwu95/MVFNet">Code</a> is available.</em></li>



    <li> <strong>07/2020: </strong> 
     <em>Our <a href="https://arxiv.org/pdf/2012.02994.pdf">ADD-GCN</a> for multi-label image recognition, 
      is accepted by 
      <strong><a href="https://eccv2020.ecva.net/">
          <font color="DarkRed">ECCV 2020</font></a></strong>. 
        <a href="https://github.com/Yejin0111/ADD-GCN">Code</a> is available.</em></li>

    <li> <strong>05/2020: </strong> 
     <em>One <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.pdf">dynamic video inference</a> paper 
      is accepted for <b>Oral Presentation</b> on <font color="Green">CVPR2020 EDLCV workshop</font>.</em></li>

    <li> <strong>07/2019: </strong>
     <em>My <strong>first</strong> paper <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf">MARL</a>, 
      a novel video sampling method, is accepted as <font color="Red"><b>Oral Presentation</b></font> (<strong>Top 4%</strong>) on 
      <strong><a href="https://iccv2019.thecvf.com/">
          <font color="DarkRed">ICCV 2019</font></a></strong>.</em></li>

    <li> <strong>09/2017: </strong> 
      <em>Recommended to <a href='http://english.ucas.ac.cn/'>University of Chinese Academy of Sciences</a> towards the Master degree.</em></li>      
    <li> <strong>06/2017: </strong>
      <em>Graduated from Central South University with the Outstanding Graduate Honor.</em></li>
    <li> <strong>10/2016: </strong> 
      <em>Joined <a href='http://mmlab.siat.ac.cn'>MMLab@Shenzhen</a> as research intern. Started doing research on Computer
      Vision.</em></li>

    </ul>
  </div>
</div>
</div>





  <div style="clear: both;">
    <div class="section">
      <h2>Industrial Experience</h2>
      <div class="paper2">
 
  <div class="experience"><img class="experience" src="logo/baidu.png" />
    <div>
      <span style="font-size: 16px;" class="h1"></b>
      <b><a href="http://research.baidu.com/">
          <font color="Brown">Baidu Research</font>
        </a></b> →
      <b><a href="http://vis.baidu.com/">
        <font color="Brown">Baidu VIS</font></a></b></span>
      <br>
      <p style="margin-top:6px;"><strong>Intern → Senior Researcher → Intern</strong> on Video Understanding & Editing
        <br><em>worked with <a href="https://jingdongwang2017.github.io">Dr. Jingdong Wang</a> and <a
          href="https://scholar.google.com/citations?hl=zh-CN&user=1wzEtxcAAAAJ">Dr. Errui Ding</a></em><br>
        Oct. 2018 - Present
      </p>
     </div>
  </div>

  <!-- <div class="experience"><img class="experience" src="logo/sensetime.png" />
    <div>
      <span style="font-size: 16px;" class="h1"></b>
        <b><a href="https://www.sensetime.com/en">
            <font color="Brown">SenseTime Research</font>
          </a></b></span>
      <br>
      <p style="margin-top:6px;"><strong>Research Intern</strong> in BigVideo Team
        <br><em>worked with <a href="https://scholar.google.com/citations?user=eGD0b7IAAAAj">Dr. Kai Chen</a></em><br>
        Jan. 2020 - Feb. 2020
      </p>
    </div>
  </div> -->

  <!-- <div class="experience"><img class="experience" src="logo/baidu_idl.png" />
    <div>
      <span style="font-size: 16px;" class="h1"></b>
        <b><a href="http://research.baidu.com/">
            <font color="Brown">Baidu Research</font>
          </a></b></span>
      <br>
      <p style="margin-top:6px;"><strong>Research Intern</strong> in Institue of Deep Learning (IDL)
        <br><em>hosted by <a href="https://scholar.google.com/citations?hl=zh-CN&user=zKtYrHYAAAAJ">Shilei Wen</a> 
          and <a href="https://scholar.google.com/citations?hl=zh-CN&user=1wzEtxcAAAAJ">Dr. Errui Ding</a></em><br>
        Oct. 2018 - Jan. 2020
      </p>
    </div>
  </div> -->

  <div class="experience"><img class="experience" src="logo/iqiyi.png" />
    <div>
      <span style="font-size: 16px;" class="h1"></b>
        <b><a href="https://ir.iqiyi.com/">
            <font color="Brown">iQIYI</font>
          </a></b></span>
      <br>
      <p style="margin-top:6px;"><strong>Research Intern</strong> in Video Analysis Group
        <br><em>hosted by </a>Qiyue Liu</a></em><br>
        Jun. 2018 - Oct. 2018
      </p>
    </div>
  </div>

  <div class="experience"><img class="experience" src="logo/Samsung.png" />
    <div>
      <span style="font-size: 16px;" class="h1"></b>
        <b><a href="https://research.samsung.com/">
            <font color="Brown">Samsung Research China</font>
          </a></b></span>
      <br>
      <p style="margin-top:6px;"><strong>Research Intern</strong> in Machine Learning Lab
        <br><em>hosted by <a href="https://www.linkedin.com/in/%E7%BD%97%E6%8C%AF%E6%B3%A2-07b971166/">Zhenbo luo</a></em><br>
        Mar. 2018 - Jun. 2018
      </p>
    </div>
  </div>

      </div>
    </div>
  </div>


  <div style="clear: both;">
    <div class="section">
      <h2>Education</h2>
      <div class="paper2">

        <div class="education"><img class="education" src="logo/usyd.png" />
          <div>
            <span style="font-size: 16px;" class="h1"></b>
              <b><a href="https://www.sydney.edu.au/">
                  <font color="Brown">The University of Sydney</font></a>, Australia</b></span>
            <br>
            <p style="margin-top:6px;"><strong>Doctor of Philosophy (Ph.D.) Candidate </strong> in Computer Science
              <br><em>Team: <a href='https://sigmalab-usyd.github.io/'>Multimedia Laboratory, Sydney (MMLab@Sydney)</a></em>
              <br><em>Advisors: <a href='https://wlouyang.github.io/'>Prof. Wanli Ouyang</a>, <a href='http://changxu.xyz/'>Prof. Chang Xu</a></em><br>
              2022 - Expected 2025
            </p>
          </div>
        </div>


        <div class="education"><img class="education" src="logo/ucas.jpeg" />
          <div>
            <span style="font-size: 16px;" class="h1"></b>
              <b><a href="https://english.ucas.ac.cn/">
                  <font color="Brown">University of Chinese Academy of Sciences</font></a>, China</b></span>
            <br>
            <p style="margin-top:6px;"><strong>Master of Science in Engineering</strong> in Pattern Recognition & Intelligent System
              <br><em>Team: <a href='http://mmlab.siat.ac.cn'>Multimedia Laboratory, SIAT, CAS (MMLab@Shenzhen)</a></em>
              <br><em>Advisors: <a href='https://scholar.google.com/citations?user=6X77S3cAAAAJ&hl=en'>Prof. Shifeng Chen</a>, 
                 <a href='http://mmlab.siat.ac.cn/yuqiao/'>Prof. Yu Qiao</a></em><br>  
              2017 - 2020
            </p>
          </div>
        </div>
  
        <div class="education"><img class="education" src="logo/csu.png" />
          <div>
            <span style="font-size: 16px;" class="h1"></b>
              <b><a href="https://en.csu.edu.cn/">
                  <font color="Brown">Central South University</font></a>, China</b></span>
            <br>
            <p style="margin-top:6px;"><strong>Bachelor of Engineering</strong> in Automation
              <br><em>Advisors: <a href='http://mmlab.siat.ac.cn/sfchen'>Prof. Shifeng Chen</a>,
                <a href='http://mmlab.siat.ac.cn/yuqiao/'>Prof. Yu Qiao</a></em> while visiting CAS (Oct. 2016 - Jun. 2017)<br>
              2013 - 2017
            </p>
          </div>
        </div>
    
      </div>
    </div>
  </div>





<a name="pub"></a>
<h2 id="confpapers">Selected Publications [ <a href="http://whwu95.github.io/publication.html">Full List</a> ] </h2>
<b>( *Co-first Author, <sup><span lang="EN-US"
      style="mso-bidi-font-size:8pt;font-family:Wingdings;mso-ascii-font-family:'Times New Roman';mso-hansi-font-family:'Times New Roman';mso-char-type:symbol;mso-symbol-font-family:Wingdings">*</span></sup>Correspondence)</b>

<div class="section">
  <div class="bio">

    <!-- ICCV23 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/ICCV2023/ATM.png" />
      <div>
        <a><b>What Can Simple Arithmetic Operations Do for Temporal Modeling?</b></a><br />
        <u><b style="color:darkred">Wenhao Wu</b></u>, Yuxin Song, Zhun Sun, Jingdong Wang, Chang Xu, Wanli Ouyang<br />
        <i>IEEE International Conference on Computer Vision <b><font color="DarkRed">(ICCV)</font></b>, 2023</i><br />
        [ <a href='https://arxiv.org/pdf/2307.08908.pdf'>PDF</a> ]
      </div>
      <div class="spanner"></div>
    </div>



    <!-- ICCV23 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/ICCV2023/UA.png" />
      <div>
        <a><b>UATVR: Uncertainty-Adaptive Text-Video Retrieval</b></a><br />
        Bo Fang*, <u><b style="color:darkred">Wenhao Wu*</b></u>, Chang Liu, Yu Zhou, Yuxin Song,
        Weiping Wang, Xiangbo Shu, Xiangyang Ji, Jingdong Wang<br />
        <i>IEEE International Conference on Computer Vision <b><font color="DarkRed">(ICCV)</font></b>, 2023</i><br />
        [ <a href='https://arxiv.org/pdf/2301.06309.pdf'>PDF</a> ]
      </div>
      <div class="spanner"></div>
    </div>

    <!-- CVPR23 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/CVPR2023/cap4video.png" />
      <div>
        <a><b>Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?</b></a><br />
        <u><b style="color:darkred">Wenhao Wu</b></u>, Haipeng Luo, Bo Fang, Jingdong Wang, Wanli Ouyang<br />
        <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
            <font color="DarkRed">(CVPR)</font>
          </b>, 2023 </i><br />
        <b><font color="Red">[Highlight, Top 2.5% of 9155 submissions]</font></b>
        [ <a href='https://arxiv.org/pdf/2301.00184.pdf'>PDF</a> ]
        [ <a href='https://github.com/whwu95/Cap4Video'>Code</a> ]
        <br /><strong>Cap4Video leverages auxiliary captions generated by GPT-2 to enhance cross-modal learning.</strong>
      </div>
      <div class="spanner"></div>
    </div>

    <!-- CVPR23-->
    <div class="paper" id="xxx"><img class="paper" src="papers/CVPR2023/BIKE.png" />
      <div>
        <a><b>Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained
            Vision-Language
            Models</b></a><br />
        <u><b style="color:darkred">Wenhao Wu</b></u>, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli
        Ouyang<br />
        <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
            <font color="DarkRed">(CVPR)</font>
          </b>, 2023 </i><br />
        [ <a href='https://arxiv.org/pdf/2301.00182.pdf'>PDF</a> ]
        [ <a href='https://github.com/whwu95/BIKE'>Code</a> ]
      </div>
      <div class="spanner"></div>
    </div>


    <!-- AAAI2023 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/AAAI2023/text4vis.png" />
      <div>
        <a><b>Revisiting Classifier: Transferring Vision-Language Models for Video Recognition</b></a><br />
        <u><b style="color:darkred">Wenhao Wu</b></u>, Zhun Sun, Wanli Ouyang<br />
        <i>The AAAI Conference on Artificial Intelligence <b>
            <font color="DarkRed">(AAAI)</font>
          </b>, 2023</i> <br />
        [ <a href='https://arxiv.org/pdf/2207.01297.pdf'>PDF</a> ]
        [ <a href='https://github.com/whwu95/Text4Vis'>Code</a> ]
        [ <a href="papers/AAAI2023/text4vis_AAAI23_Poster_Wenhao.pdf">Poster</a> ]
        [ <a href="papers/AAAI2023/text4vis-aaai2023-presentation.pdf">Slides</a> ]
        [ <a href=''>Video</a> ] <br />
        <strong>We revisit the classifier with the textual embeddings, and achieve SOTA performance on
          Full-supervision/Few-shot/Zero-shot recognition.</strong>
      </div>
      <div class="spanner"></div>
    </div>

    <!-- ECCV2022-->
    <div class="paper" id="xxx"><img class="paper" src="papers/ECCV2022/nsnet.png" />
      <div>
        <a><b>NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition</b></a><br />
        Boyang Xia*, <u><b style="color:darkred">Wenhao Wu</u>*<sup><span lang="EN-US"
            style="mso-bidi-font-size:8pt;font-family:Wingdings;mso-ascii-font-family:'Times New Roman';mso-hansi-font-family:'Times New Roman';mso-char-type:symbol;mso-symbol-font-family:Wingdings">*</span></sup></b>,
        Haoran Wang, Rui Su,
        Dongliang He, Haosen Yang,
        Xiaoran Fan, Wanli Ouyang<br />
        <i>European Conference on Computer Vision<b>
            <font color="DarkRed">(ECCV)</font>
          </b>, 2022</i> <br />
        [ <a href='https://arxiv.org/pdf/2207.10388.pdf'>PDF</a> ]
        [ <a href='https://lawrencexia2008.github.io/projects/nsnet'>Project</a> ] <br />
        <strong>A sampler with a 4x faster practical speed than SOTA methods.</strong>
      </div>
      <div class="spanner"></div>
    </div>


    <!-- ECCV2022-->
    <div class="paper" id="xxx"><img class="paper" src="papers/ECCV2022/tsqnet.png" />
      <div>
        <a><b>Temporal Saliency Query Network for Efficient Video Recognition</b></a><br />
        Boyang Xia*, Zhihao Wang*, <u><b style="color:darkred">Wenhao Wu</u><sup><span lang="EN-US"
            style="mso-bidi-font-size:8pt;font-family:Wingdings;mso-ascii-font-family:'Times New Roman';mso-hansi-font-family:'Times New Roman';mso-char-type:symbol;mso-symbol-font-family:Wingdings">*</span></sup></b>,
        Haoran
        Wang, Jungong Han<br />
        <i>European Conference on Computer Vision<b>
            <font color="DarkRed">(ECCV)</font>
          </b>, 2022</i> <br />

        [ <a href='https://arxiv.org/pdf/2207.10379.pdf'>PDF</a> ]
        [ <a href='https://lawrencexia2008.github.io/projects/tsqnet'>Project</a> ] <br />
        <strong>TSQNet, the first work to model temporal sampling as a query-response task.</strong>
      </div>
      <div class="spanner"></div>
    </div>

    <!-- ACMMM2022-->
    <div class="paper" id="xxx"><img class="paper" src="papers/ACMMM2O22/mamico.png" />
      <div>
        <a><b>MaMiCo: Macro-to-Micro Semantic Correspondence for Self-supervised Video Representation
            Learning</b></a><br />
        Bo Fang*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Chang Liu*, Yu Zhou, Dongliang He, Weiping
        Wang<br />
        <i>ACM International Conference on Multimedia<b>
            <font color="DarkRed">(ACMMM)</font>
          </b>, 2022</i> <br />
        <b>
          <font color="Red">[Oral, 5.0% acceptance rate]</font>
        </b>
        [ <a href='https://dl.acm.org/doi/10.1145/3503161.3547888'>PDF</a> ]
        [ <a href=''>Project</a> ] <br />
        <strong>MaMiCo, a self-supervised Macro-to-Micro Semantic Correspondence learning framework for video
          representation learning.</strong>
      </div>
      <div class="spanner"></div>
    </div>

    <!-- AAAI2022-->
    <div class="paper" id="xxx"><img class="paper" src="papers/AAAI2022/BCNet.png" />
      <div>
        <a><b>Temporal Action Proposal Generation with Background Constraint</b></a><br />
        Haosen Yang*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Lining Wang, Sheng Jin, Boyang Xia,
        Hongxun Yao, Hujie
        Huang<br />
        <i>The AAAI Conference on Artificial Intelligence <b>
            <font color="DarkRed">(AAAI)</font>
          </b>, 2022</i> <br />
        <b>
          <font color="Green">[15% acceptance rate]</font>
        </b>
        [ <a href='https://arxiv.org/pdf/2112.07984.pdf'>PDF</a> ]
        [ <a href='https://github.com/happy-lifi/BCNet'>Code</a> ] <br />
        <strong>BCNet, an general framework for effective Temporal Action Proposal Generation.</strong>
      </div>
      <div class="spanner"></div>
    </div>


    <!-- ICCV2021 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/ICCV2021/ASCNet.png" />
      <div>
        <a><b>ASCNet: Self-supervised Video Representation Learning with Appearance-Speed
            Consistency</b></a><br />
        Deng Huang*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Weiwen Hu, Xu Liu, Dongliang He, Zhihua Wu,
        Xiangmiao
        Wu, Mingkui Tan, Errui Ding <br />
        <i>IEEE International Conference on Computer Vision <b>
            <font color="DarkRed">(ICCV)</font>
          </b>, 2021 </i><br />
        [ <a href='https://arxiv.org/pdf/2106.02342.pdf'>PDF</a> ]
        [ <a href="papers/ICCV2021/ICCV2021_Poster.pdf">Poster</a> ]
        [ <a href="papers/ICCV2021/iccv2021_presentation_5min_final.pdf">Slides</a> ]
        [ <a
          href='https://www.bilibili.com/video/BV1rg411F7MM/?spm_id_from=333.999.0.0&vd_source=b8bdec8f8b43fac4f439e3535c6aa33e'>Video</a>
        ]
        [ <a href=''>Code</a> ] <br />
        <strong>An effective self-supervised video representation learning framework.</strong>
      </div>
      <div class="spanner"></div>
    </div>


  <!-- ACMMM2021 -->
  <div class="paper" id="xxx"><img class="paper" src="papers/ACMMM2021/DSANet.png" />
    <div>
      <a><b>DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning</b></a><br />
      <u><b style="color:darkred">Wenhao Wu</u>*</b>, Yuxiang Zhao*, Yanwu Xu, Xiao Tan, Dongliang He, Zhikang
      Zou, Jin
      Ye, Yingying Li, Mingde Yao, Zichao Dong,
      Yifeng Shi <br />
      <i> ACM International Conference on Multimedia <b>
          <font color="DarkRed">(ACMMM)</font>
        </b>, 2021</i> <br />
      [ <a href='https://arxiv.org/pdf/2105.12085.pdf'>PDF</a> ]
      [ <a href="papers/ACMMM2021/DSANet_poster.pdf">Poster</a> ]
      [ <a href="papers/ACMMM2021/DSANet.pdf">Slides</a> ]
      [ <a href='https://github.com/whwu95/DSANet'>Code</a> ] <br />
      <strong>An efficient plug-and-play module for effective video-level representation learning.</strong>
    </div>
    <div class="spanner"></div>
  </div>

  <!-- AAAI2021 -->
  <div class="paper" id="AAAI2021"><img class="paper" src="papers/AAAI2021/MVF.png" />
    <div>
      <a><b>MVFNet: Multi-View Fusion Network for Efficient Video Recognition</b></a><br />
      <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan, Errui Ding
      <br />
      <i>The AAAI Conference on Artificial Intelligence <b>
          <font color="DarkRed">(AAAI)</font>
        </b>, 2021</i> <br />
      [ <a href='https://arxiv.org/pdf/2012.06977.pdf'>PDF</a> ]
      [ <a href="papers/AAAI2021/MVFNet_AAAI21_Poster_Wenhao.pdf">Poster</a> ]
      [ <a href="papers/AAAI2021/mvfnet-aaai2021-presentation.pdf">Slides</a> ]
      [ <a href='https://github.com/whwu95/MVFNet'>Code</a> ]
      [ <a shape="rect" href="javascript:togglebib(&#39;AAAI2021&#39;)" class="togglebib">Bibtex</a> ]<br />
      <pre xml:space="preserve" style="display: none;">
          @inproceedings{wu2021mvfnet,
          title={Mvfnet: Multi-view fusion network for efficient video recognition},
          author={Wu, Wenhao and He, Dongliang and Lin, Tianwei and Li, Fu and Gan, Chuang and Ding, Errui},
          booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
          volume={35},
          number={4},
          pages={2943--2951},
          year={2021}
          }
              </pre>
      <strong>An efficient architecture for video recognition based on 2D CNN.</strong>
    </div>
    <div class="spanner"></div>
  </div>

  <!-- CVPRW2021 -->
  <div class="paper" id="CVPRW2021"><img class="paper" src="papers/CVPRW2021/aicity.png" />
    <div>
      <a><b>Good Practices and A Strong Baseline for Traffic Anomaly Detection</b></a><br />
      Yuxiang Zhao*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Yue He, Yingying Li, Xiao Tan, Shifeng
      Chen <br />

      <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
          <font color="DarkRed">(CVPR)</font>
        </b> - 5th AI City Challenge (AICity), 2021</i> <br />

      [ <a href='https://arxiv.org/abs/2105.03827'>PDF</a> ]
      [ <a href=''>Code</a> ]
      [ <a shape="rect" href="javascript:togglebib(&#39;CVPRW2021&#39;)" class="togglebib">Bibtex</a> ]<br />
      <pre xml:space="preserve" style="display: none;">
        @inproceedings{zhao2021good,
        title={Good Practices and A Strong Baseline for Traffic Anomaly Detection},
        author={Zhao, Yuxiang and Wu, Wenhao and He, Yue and Li, Yingying and Tan, Xiao and Chen, Shifeng},
        booktitle={Proceedings of CVPR Workshops},
        year={2021}
        }
          </pre>
      <alert>Winner of AI City challenge for traffic anomaly detection</alert>
    </div>
    <div class="spanner"></div>
  </div>



<!-- CVPRW2020 -->
<div class="paper" id="CVPRW2020"><img class="paper" src="papers/CVPRW2020/teaser.gif" />
  <div>
    <a><b>Dynamic Inference: A New Approach Toward Efficient Video Action Recognition</b></a><br />
    <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, Shilei Wen
    <br />
    <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
        <font color="DarkRed">(CVPR)</font>
      </b> - Joint Workshop on
      Efficient Deep Learning in Computer Vision (EDLCV), 2020 </i><br />

    <b>
      <font color="Red">[Oral]</font>
    </b>
    [ <a
      href='http://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.html'>PDF</a>
    ]
    [ <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>Slides</a> ]
    [ <a shape="rect" href="javascript:togglebib(&#39;CVPRW2020&#39;)" class="togglebib">Bibtex</a> ]<br />
    <pre xml:space="preserve" style="display: none;">
        @inproceedings{wu2020dynamic,
            title={Dynamic Inference: A New Approach Toward Efficient Video Action Recognition},
            author={Wu, Wenhao and He, Dongliang and Tan, Xiao and Chen, Shifeng 
              and Yang, Yi and Wen, Shilei},
            booktitle={Proceedings of CVPR Workshops},
            pages={676--677},
            year={2020}
        }
            </pre>
  </div>
  <div class="spanner"></div>
</div>


<!-- ICCV2019 -->
<div class="paper" id="ICCV19"><img class="paper" src="papers/ICCV2019/ICCV2019.png" />
  <div><b><a>Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video
        Recognition</a></b><br />
    <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Shilei Wen <br />
    <i>IEEE International Conference on Computer Vision <b><font color="DarkRed">(ICCV)</font></b>, 2019 </i><br /><b>
      <font color="Red">[Oral, 4.3% acceptance rate]</font></b>
    [ <a
      href='http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf'>PDF</a>
    ]
    [ <a href="papers/ICCV2019/MARL_ICCV19_Poster_Wenhao_Wu.pdf">Poster</a> ]
    [ <a href="papers/ICCV2019/ICCV19_Oral_5min.pdf">Slides</a> ]
    [ <a shape="rect" href="javascript:togglebib(&#39;ICCV19&#39;)" class="togglebib">Bibtex</a> ]<br />
    <pre xml:space="preserve" style="display: none;">
@inproceedings{wu2019multi,
    title={Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed
       Video Recognition},
    author={Wu, Wenhao and He, Dongliang and Tan, Xiao and Chen, Shifeng and Wen, Shilei},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    pages={6222--6231},
    year={2019}
}
</pre>
  </div>
  <div class="spanner"></div>

</div>
</div>










<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Contests</h2>
<div class="paper">
<ul>
<!-- <li>The NTIRE Perceptual Extreme Super-Resolution Challenge on CVPR'20: Won 1st in SSIM, 2nd in PSNR, and 11th in LPIPS.</li> -->
<li>CVPR2021 AI CITY Challenge: Traffic Anomaly Detection, <font color="Red"><b>Winner Award</b></font>, 2021</li>
<li>CVPR2021 NTIRE Challenge on Image Deblurring: Track 2 JPEG Artifacts, <font color="Blue"><b>Runner-Up Award</b></font>, 2021</li>
<!-- <li>The ActivityNet Challenge 2018 at CVPR'18: Trimmed Action Recognition (Kinetics-600), Rank: 8</li> -->
<li>The First-class Prize of America Mathematical Contest in Modeling (MCM), 2016 </li>
<li>The First-class Prize in National Undergraduate Mechanical Innovation Design Competition, 2016 </li>
<li>The Second-class Prize in China Freescale Cup Intelligent Car Competition (South China Region), 2015 </li>
<li>The Second-class Prize in Smart Car Racing Competition of Hunan Province, 2015</li>



</ul>
<div class="spanner"></div>
</div>
</div>
</div>







<div style="clear: both;">
<div class="section"><h2>Awards</h2>
<div class="paper">
<h3>PhD:</h3>
<li><a href="https://good-design.org/projects/curbyit/">Australian Good Design Award</a></li>
<li> Faculty of Engineering Research Scholarship at the University of Sydney (<b>Tuition fees offsets + $37,207 p.a.</b>)</li>
<h3>Master:</h3>
<li> Baidu Best Newcomer, 2021</li>
<li> Excellent Student Cadre of University of Chinese Academy of Sciences, 2020
<li> Baidu Best Intern, 2019 </li>
<li> Scholarship for Academic Excellence of Shenzhen Institutes of Advanced Technology, CAS, 2018 </li>
<li> University of Chinese Academy of Sciences (UCAS) Scholarships (<b>16,000 RMB p.a.</b>), 2017-2020</li>
<h3>Undergraduate:</h3>
<li> Excellent Undergraduate Student of Central South University, 2017 </li>
<li> Outstanding Student of Central South University, 2016 </li>
<li> National Endeavor Scholarship (Top 5%), 2016 </li>
<li> Excellent National College Students Innovation and Entrepreneurship Project (20,000 RMB), 2016</li>
<li> Excellent Student Cadre of Central South University, 2015</li>
<li> Excellent League Member of Central South University, 2015</li>
<li> Scholarship for Academic Excellence of Central South University, 2014/2015/2016</li>

</div>
</div>
</div>

  
<div style="clear: both;">
<div class="section"><h2>Academic Activities</h2>
<div class="paper">
<h3>Journal Reviewer</h3>
<li>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>)</li>
<li>IEEE Transaction on Image Processing (<b>TIP</b>)</li>
<li>IEEE Transaction on Circuits and Systems for Video Technology (<b>TCSVT</b>)</li>
<li>IEEE Transactions on Multimedia (<b>TMM</b>)</li>
<li>IEEE Transactions on Biomedical Engineering (<b>TBME</b>)</li>
<li>Knowledge-Based Systems</li>

<h3>Conference PC Member/Reviewer</h3>
<li>Reviewer, The Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021, 2022, 2023</li>
<li>Reviewer, International Conference on Computer Vision (<b>ICCV</b>), 2023</li>
<li>Reviewer, European Conference on Computer Vision (<b>ECCV</b>), 2022</li>
<li>PC Member, International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2021</li>
<li>PC Member, The AAAI Conference on Artifical Intelligence (<b>AAAI</b>), 2021, 2022</li>
<li>Reviewer, ACM International Conference on Multimedia (<b>ACMMM</b>), 2023</li>
<li>Reviewer, Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2022</li>
<li>Reviewer, International Conference on Digital Image Computing: Techniques and Applications (<b>DICTA</b>), 2022</li>

<h3>Member of IEEE, ACM, AAAI and CVF</h3>

</div>
</div>
</div>  


<div style="clear: both;">
  <div class="section">
    <h2>Current/Past Mentoring</h2>
    <div class="paper">
      <a href=''>Yuxiang Zhao</a> (Peking University),
      <a href='https://zhaohengyuan1.github.io/'>Hengyuan Zhao</a> (NUS),
      <a href='https://scholar.google.com.hk/citations?user=rSuLEXwAAAAJ&hl=en'>Deng Huang</a> (SCUT),
      <a href='https://github.com/happy-lifi'>Haosen Yang</a> (University of Surrey),
      <a href='https://lawrencexia2008.github.io'>Boyang Xia</a> (ICT, CAS),
      <a href=''>Zhihao Wang</a> (ICT, CAS),
      <a href='https://bofang98.github.io/'>Bo Fang</a> (IIE, CAS),
      <a href=''>Yuguo Wang</a> (Duke University),
      <a href=''>Haipeng Luo</a> (SIAT, CAS),
      <a href=''>Huanjin Yao</a> (Tsinghua University)


    </div>
  </div>
</div>

<div style="clear: both;">
<div class="section"><h2>Collaborators & Friends</h2>
<div class="paper">
<a href='https://scholar.google.com/citations?user=iGA10XoAAAAJ&hl=zh-CN'>Xiaohan Wang</a> (Zhejiang University),
<a href='https://www.linkedin.com/in/xiao-tan-46b70a85/'>Xiao Tan</a> (Baidu),
<a href='https://www.linkedin.com/in/dongliang-he-6b926077'>Dongliang He</a> (Baidu),
<a href='https://wzmsltw.github.io/'>Tianwei Lin</a> (Baidu),
<a href='https://xuyanwu.github.io/'>Yanwu Xu</a> (PITT),
<a href='https://wujie1010.github.io/'>Jie Wu</a> (Bytedance),
<a href='https://yejin0111.github.io/'>Jin Ye</a> (Shanghai Lab),
<a href='https://people.csail.mit.edu/ganchuang/'>Chuang Gan</a> (MIT-IBM Watson Lab),
<a href='https://scholar.google.com.hk/citations?user=WRIYcNwAAAAJ&hl=zh-CN'>Yihao Liu</a> (SIAT, CAS),
<a href=''>Chang Liu</a> (Tsinghua University),
<a href='https://scholar.google.co.jp/citations?user=Y-3iZ9EAAAAJ&hl=en'>Zhun Sun</a> (Baidu)






</div>
</div>
</div>


<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 10th May, 2023</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<div id="clustrmaps-widget"></div><script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=0e1633&w=300&t=tt&d=N0ZXDEaZVrn2LXkG_byNAa2NLm2v6WRQIUifhg-2f1A&co=0b4975&ct=cdd4d9&cmo=3acc3a&cmn=ff5353'></script>

</body>
</html>